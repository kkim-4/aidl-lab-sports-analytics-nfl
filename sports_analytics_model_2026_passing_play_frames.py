# -*- coding: utf-8 -*-
"""Sports_Analytics_Model_2026_Passing_Play_Frames.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LPALrsWXa5-z-k8iRNdYOleP-j9zAwHi
"""

from google.colab import drive
import os
import glob
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers
import keras.ops as ops
import matplotlib.pyplot as plt
import matplotlib.patches as patches

# 1. Mount Google Drive
drive.mount('/content/drive')

# 2. Set Paths & Constants
DATA_PATH = '/content/drive/MyDrive/BDB2026_Data/'
os.chdir(DATA_PATH)

TRACK_SHAPE = (22, 10, 6) # 22 players, 10 frames, 6 features
CONT_DIM = 5              # Normalized context: [bl_x, bl_y, yardline, s, a]
BATCH_SIZE = 32

def bdb_delta_generator(file_pattern_in, file_pattern_out, batch_size=32):
    in_files = sorted(glob.glob(file_pattern_in))
    out_files = sorted(glob.glob(file_pattern_out))

    while True:
        for f_in, f_out in zip(in_files, out_files):
            df_in = pd.read_csv(f_in); df_out = pd.read_csv(f_out)

            # Map direction context
            dir_map = df_in[['game_id', 'play_id', 'play_direction']].drop_duplicates()
            df_out = df_out.merge(dir_map, on=['game_id', 'play_id'], how='left')

            # Standardize one column at a time
            for df in [df_in, df_out]:
                mask = df['play_direction'] == 'left'
                df.loc[mask, 'x'] = 120.0 - df.loc[mask, 'x']
                df.loc[mask, 'y'] = 53.3 - df.loc[mask, 'y']
                if 'ball_land_x' in df.columns:
                    df.loc[mask, 'ball_land_x'] = 120.0 - df.loc[mask, 'ball_land_x']
                    df.loc[mask, 'ball_land_y'] = 53.3 - df.loc[mask, 'ball_land_y']

            # Delta Calculation
            df_out = df_out.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])
            df_out['dx'] = df_out.groupby(['game_id', 'play_id', 'nfl_id'])['x'].diff().fillna(0)
            df_out['dy'] = df_out.groupby(['game_id', 'play_id', 'nfl_id'])['y'].diff().fillna(0)

            all_play_ids = df_in[['game_id', 'play_id']].drop_duplicates().values
            X_track_batch, X_cont_batch, Y_delta_batch = [], [], []

            for g_id, p_id in all_play_ids:
                p_in = df_in[(df_in['game_id'] == g_id) & (df_in['play_id'] == p_id)]
                p_out = df_out[(df_out['game_id'] == g_id) & (df_out['play_id'] == p_id)]

                target_mask = p_in['player_to_predict'] == True
                if not target_mask.any(): continue
                target_nfl_id = p_in.loc[target_mask, 'nfl_id'].iloc[0]

                y_deltas = p_out[p_out['nfl_id'] == target_nfl_id].sort_values('frame_id')[['dx', 'dy']].values
                if len(y_deltas) < 25: continue

                # --- FIX: Populate the Tracking Tensor ---
                track_tensor = np.zeros((22, 10, 6))
                player_ids = p_in['nfl_id'].unique()[:22]

                for idx, pid in enumerate(player_ids):
                    p_data = p_in[p_in['nfl_id'] == pid].sort_values('frame_id')
                    f_count = min(len(p_data), 10)
                    # Normalized features: x/120, y/53.3, s/12, a/10, etc.
                    track_tensor[idx, :f_count, 0] = p_data['x'].values[:f_count] / 120.0
                    track_tensor[idx, :f_count, 1] = p_data['y'].values[:f_count] / 53.3
                    track_tensor[idx, :f_count, 2] = p_data['s'].values[:f_count] / 12.0
                    track_tensor[idx, :f_count, 3] = p_data['a'].values[:f_count] / 10.0
                    # Relative distance to ball landing spot
                    track_tensor[idx, :f_count, 4] = (p_data['ball_land_x'].values[:f_count] - p_data['x'].values[:f_count]) / 120.0
                    track_tensor[idx, :f_count, 5] = (p_data['ball_land_y'].values[:f_count] - p_data['y'].values[:f_count]) / 53.3

                # Build Context
                ctx_row = p_in.loc[target_mask].iloc[-1]
                cont_vec = np.array([
                    ctx_row['ball_land_x'] / 120.0,
                    ctx_row['ball_land_y'] / 53.3,
                    ctx_row['absolute_yardline_number'] / 100.0,
                    ctx_row['s'] / 12.0,
                    ctx_row['a'] / 10.0
                ])

                X_track_batch.append(track_tensor)
                X_cont_batch.append(cont_vec)
                # Labels are deltas (how much they move per frame)
                Y_delta_batch.append(y_deltas[:25])

                if len(X_track_batch) == batch_size:
                    yield (np.array(X_track_batch), np.array(X_cont_batch)), np.array(Y_delta_batch)
                    X_track_batch, X_cont_batch, Y_delta_batch = [], [], []

import tensorflow as tf
from tensorflow.keras import layers
import keras.ops as ops

class AxialBlock(layers.Layer):
    def __init__(self, d_model, heads=8):
        super().__init__()
        self.mha_spatial = layers.MultiHeadAttention(num_heads=heads, key_dim=d_model)
        self.mha_temporal = layers.MultiHeadAttention(num_heads=heads, key_dim=d_model)
        self.norm = layers.LayerNormalization()

    def call(self, x):
        b, p, t, f = ops.shape(x)[0], ops.shape(x)[1], ops.shape(x)[2], ops.shape(x)[3]
        # Spatial Pass
        x_s = ops.transpose(x, axes=[0, 2, 1, 3])
        x_s = ops.reshape(x_s, (-1, p, f))
        x_s = self.mha_spatial(x_s, x_s)
        x = x + ops.transpose(ops.reshape(x_s, (b, t, p, f)), axes=[0, 2, 1, 3])
        # Temporal Pass
        x_t = ops.reshape(x, (-1, t, f))
        x_t = self.mha_temporal(x_t, x_t)
        return self.norm(x + ops.reshape(x_t, (b, p, t, f)))

def build_model():
    track_in = layers.Input(shape=(22, 10, 6))
    cont_in = layers.Input(shape=(5,))

    x = layers.Dense(256, activation='relu')(track_in)
    x = AxialBlock(256)(x)
    x = AxialBlock(256)(x)

    x = layers.GlobalAveragePooling2D()(x)
    combined = layers.Concatenate()([x, cont_in])

    # Regression head for 25 steps of (dx, dy)
    d = layers.Dense(256, activation='relu')(combined)
    d = layers.BatchNormalization()(d)
    out = layers.Dense(50, activation='linear')(d)

    return tf.keras.Model(inputs=[track_in, cont_in], outputs=layers.Reshape((25, 2))(out))

# Re-instantiate the dataset to apply the fix
train_ds = tf.data.Dataset.from_generator(
    lambda: bdb_delta_generator('input_2023_w*.csv', 'output_2023_w*.csv', batch_size=32),
    output_signature=((tf.TensorSpec(shape=(32, 22, 10, 6), dtype=tf.float32),
                       tf.TensorSpec(shape=(32, 5), dtype=tf.float32)),
                      tf.TensorSpec(shape=(32, 25, 2), dtype=tf.float32))).prefetch(tf.data.AUTOTUNE)

# Run a test plot
(test_X, test_C), test_Y_true = next(iter(train_ds))
test_Y_pred = model.predict([test_X, test_C])

# Call the cumulative plotter
plot_cumulative_path(test_X[0], test_Y_true[0], test_Y_pred[0], is_normalized=True)